{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating PLFS dataloader for reading txt file|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same functionality can be easily achieved by creating it as a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLFSdataloader:\n",
    "    \n",
    "    #Initialising the dataloader\n",
    "    def __init__ (self,file_path,col_names,col_desc,col_specs):\n",
    "        self.file_path = file_path\n",
    "        self.col_names = col_names\n",
    "        self.col_desc = col_desc\n",
    "        self.col_specs = col_specs\n",
    "        self.data_df = None\n",
    "        self.col_df = None\n",
    "    \n",
    "        #Automatically load data when an instance is created\n",
    "        self.load_data()\n",
    "\n",
    "    #Function to load data as a df\n",
    "    def load_data (self):\n",
    "        self.data_df = pd.read_fwf(self.file_path, colspecs = self.col_specs, names = self.col_names, dtype=str)\n",
    "        \n",
    "        col_dictionary = {\n",
    "            \"variable\" : self.col_names,\n",
    "            \"descriptions\" : self.col_desc\n",
    "        }\n",
    "        self.col_df = pd.DataFrame(col_dictionary)\n",
    "        self.col_df = self.col_df.set_index('variable')\n",
    "    \n",
    "    #Return the output as two separate dfs for data and columns\n",
    "    def get_dataframes(self):\n",
    "        return self.data_df, self.col_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading household files from first visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathlib.Path(r\"..\\Raw data\\HHV1.TXT\").resolve().exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUT DATA\n",
    "\n",
    "#file path\n",
    "file_path = r'..\\Raw data\\HHV1.TXT'\n",
    "\n",
    "#column indices\n",
    "col_specs = [(0,4),\t(4,7),\t(7,9),\t(9,11),\t(11,12),\t(12,14),\t(14,16),\t(16,19),\t(19,21),\t(21,23),\t(23,24),\t(24,28),\t(28,33),\t(33,34),\t(34,35),\t(35,37),\t(37,39),\t(39,40),\t(40,41),\t(41,42),\t(42,44),\t(44,45),\t(45,46),\t(46,47),\t(47,55),\t(55,63),\t(63,71),\t(71,79),\t(79,87),\t(87,95),\t(95,97),\t(97,105),\t(105,109),\t(109,112),\t(112,115),\t(115,125),\t(125,126)]\n",
    "\n",
    "#column names\n",
    "col_names = [\"hhvar1\",\t\"hhvar2\",\t\"hhvar3\",\t\"hhvar4\",\t\"hhvar5\",\t\"hhvar6\",\t\"hhvar7\",\t\"hhvar8\",\t\"hhvar9\",\t\"hhvar10\",\t\"hhvar11\",\t\"hhvar12\",\t\"hhvar13\",\t\"hhvar14\",\t\"hhvar15\",\t\"hhvar16\",\t\"hhvar17\",\t\"hhvar18\",\t\"hhvar19\",\t\"hhvar20\",\t\"hhvar21\",\t\"hhvar22\",\t\"hhvar23\",\t\"hhvar24\",\t\"hhvar25\",\t\"hhvar26\",\t\"hhvar27\",\t\"hhvar28\",\t\"hhvar29\",\t\"hhvar30\",\t\"hhvar31\",\t\"hhvar32\",\t\"hhvar33\",\t\"hhvar34\",\t\"hhvar35\",\t\"hhvar36\",\t\"hhvar37\"]\n",
    "\n",
    "#column descriptions\n",
    "col_desc = [\"File Identification\",\t\"Schdule\",\t\"Quarter\",\t\"Visit\",\t\"Sector\",\t\"State/Ut Code\",\t\"District Code\",\t\"NSS-Region\",\t\"Stratum\",\t\"Sub-Stratum\",\t\"Sub-Sample\",\t\"Fod Sub-Region\",\t\"FSU\",\t\"Sample Sg/Sb No.\",\t\"Second Stage Stratum No.\",\t\"Sample Household Number\",\t\"Month of Survey\",\t\"Response Code\",\t\"Survey Code\",\t\"Reason for Substitution of original household\",\t\"Household Size\",\t\"Household Type\",\t\"Religion\",\t\"Social Group\",\t\"Household's usual consumer Expenditure in A Month for purposes out of Goods and Services(Rs.)\",\t\"Imputed value of usual consumption in a month out of Home Grown stock (Rs.)\",\t\"Imputed value of usual consumption in a Month from wages in kind,free collection, gifts etc. (Rs.)\",\t\"Household's Annual Expenditure on purchase of items like clothing, footwear etc.(Rs.)\",\t\"Household's Annual Expenditure on purchase of durables like Bedstead, TV, fridge etc.(Rs.)\",\t\"Household'S Usual Consumer Expenditure In A Month (Rs.)\",\t\"Informant Serial no.\",\t\"Survey Date\",\t\"Total Time Taken To Canvass Sch. 10.4\",\t\"Ns count for sector x stratum x substratum x sub-sample\",\t\"Ns count for sector x stratum x substratum\",\t\"Sub-sample wise Multiplier\",\t\"Count of contributing State x Sector x Stratum x SubStratum in 4 Quarters\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiating PLFS dataloader with hh files\n",
    "df_hhv1,coldesc_hhv1 = PLFSdataloader(file_path = file_path, col_names = col_names, col_desc = col_desc, col_specs = col_specs).get_dataframes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hhvar1</th>\n",
       "      <th>hhvar2</th>\n",
       "      <th>hhvar3</th>\n",
       "      <th>hhvar4</th>\n",
       "      <th>hhvar5</th>\n",
       "      <th>hhvar6</th>\n",
       "      <th>hhvar7</th>\n",
       "      <th>hhvar8</th>\n",
       "      <th>hhvar9</th>\n",
       "      <th>hhvar10</th>\n",
       "      <th>...</th>\n",
       "      <th>hhvar28</th>\n",
       "      <th>hhvar29</th>\n",
       "      <th>hhvar30</th>\n",
       "      <th>hhvar31</th>\n",
       "      <th>hhvar32</th>\n",
       "      <th>hhvar33</th>\n",
       "      <th>hhvar34</th>\n",
       "      <th>hhvar35</th>\n",
       "      <th>hhvar36</th>\n",
       "      <th>hhvar37</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FVH7</td>\n",
       "      <td>104</td>\n",
       "      <td>Q1</td>\n",
       "      <td>V1</td>\n",
       "      <td>1</td>\n",
       "      <td>02</td>\n",
       "      <td>04</td>\n",
       "      <td>021</td>\n",
       "      <td>01</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>10000</td>\n",
       "      <td>0</td>\n",
       "      <td>8733</td>\n",
       "      <td>03</td>\n",
       "      <td>11082023</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>246798</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FVH7</td>\n",
       "      <td>104</td>\n",
       "      <td>Q1</td>\n",
       "      <td>V1</td>\n",
       "      <td>1</td>\n",
       "      <td>02</td>\n",
       "      <td>04</td>\n",
       "      <td>021</td>\n",
       "      <td>01</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>9000</td>\n",
       "      <td>0</td>\n",
       "      <td>7550</td>\n",
       "      <td>03</td>\n",
       "      <td>11082023</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>35596</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FVH7</td>\n",
       "      <td>104</td>\n",
       "      <td>Q1</td>\n",
       "      <td>V1</td>\n",
       "      <td>1</td>\n",
       "      <td>02</td>\n",
       "      <td>04</td>\n",
       "      <td>021</td>\n",
       "      <td>01</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>10000</td>\n",
       "      <td>0</td>\n",
       "      <td>8483</td>\n",
       "      <td>01</td>\n",
       "      <td>11082023</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>35596</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FVH7</td>\n",
       "      <td>104</td>\n",
       "      <td>Q1</td>\n",
       "      <td>V1</td>\n",
       "      <td>1</td>\n",
       "      <td>02</td>\n",
       "      <td>04</td>\n",
       "      <td>021</td>\n",
       "      <td>01</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>8000</td>\n",
       "      <td>0</td>\n",
       "      <td>7017</td>\n",
       "      <td>01</td>\n",
       "      <td>10082023</td>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>61700</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FVH7</td>\n",
       "      <td>104</td>\n",
       "      <td>Q1</td>\n",
       "      <td>V1</td>\n",
       "      <td>1</td>\n",
       "      <td>02</td>\n",
       "      <td>04</td>\n",
       "      <td>021</td>\n",
       "      <td>01</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>8000</td>\n",
       "      <td>5000</td>\n",
       "      <td>9333</td>\n",
       "      <td>03</td>\n",
       "      <td>10082023</td>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1613680</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  hhvar1 hhvar2 hhvar3 hhvar4 hhvar5 hhvar6 hhvar7 hhvar8 hhvar9 hhvar10  ...  \\\n",
       "0   FVH7    104     Q1     V1      1     02     04    021     01      14  ...   \n",
       "1   FVH7    104     Q1     V1      1     02     04    021     01      14  ...   \n",
       "2   FVH7    104     Q1     V1      1     02     04    021     01      14  ...   \n",
       "3   FVH7    104     Q1     V1      1     02     04    021     01      14  ...   \n",
       "4   FVH7    104     Q1     V1      1     02     04    021     01      14  ...   \n",
       "\n",
       "  hhvar28 hhvar29 hhvar30 hhvar31   hhvar32 hhvar33 hhvar34 hhvar35  hhvar36  \\\n",
       "0   10000       0    8733      03  11082023      70       2       4   246798   \n",
       "1    9000       0    7550      03  11082023      60       2       4    35596   \n",
       "2   10000       0    8483      01  11082023      64       2       4    35596   \n",
       "3    8000       0    7017      01  10082023      65       2       4    61700   \n",
       "4    8000    5000    9333      03  10082023      65       2       4  1613680   \n",
       "\n",
       "  hhvar37  \n",
       "0       4  \n",
       "1       4  \n",
       "2       4  \n",
       "3       4  \n",
       "4       4  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hhv1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading personal files from first visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'..\\Raw data\\PERV1.TXT'\n",
    "col_specs = [(0,4),\t(4,7),\t(7,9),\t(9,11),\t(11,12),\t(12,14),\t(14,16),\t(16,19),\t(19,21),\t(21,23),\t(23,24),\t(24,28),\t(28,33),\t(33,34),\t(34,35),\t(35,37),\t(37,39),\t(39,40),\t(40,41),\t(41,44),\t(44,45),\t(45,47),\t(47,49),\t(49,51),\t(51,53),\t(53,54),\t(54,55),\t(55,57),\t(57,58),\t(58,59),\t(59,60),\t(60,62),\t(62,67),\t(67,70),\t(70,71),\t(71,73),\t(73,75),\t(75,76),\t(76,77),\t(77,78),\t(78,79),\t(79,80),\t(80,82),\t(82,87),\t(87,90),\t(90,92),\t(92,94),\t(94,95),\t(95,96),\t(96,97),\t(97,98),\t(98,99),\t(99,100),\t(100,101),\t(101,102),\t(102,103),\t(103,104),\t(104,105),\t(105,107),\t(107,108),\t(108,110),\t(110,112),\t(112,114),\t(114,119),\t(119,121),\t(121,123),\t(123,125),\t(125,130),\t(130,132),\t(132,134),\t(134,136),\t(136,138),\t(138,140),\t(140,145),\t(145,147),\t(147,149),\t(149,151),\t(151,156),\t(156,158),\t(158,160),\t(160,162),\t(162,164),\t(164,166),\t(166,171),\t(171,173),\t(173,175),\t(175,177),\t(177,182),\t(182,184),\t(184,186),\t(186,188),\t(188,190),\t(190,192),\t(192,197),\t(197,199),\t(199,201),\t(201,203),\t(203,208),\t(208,210),\t(210,212),\t(212,214),\t(214,216),\t(216,218),\t(218,223),\t(223,225),\t(225,227),\t(227,229),\t(229,234),\t(234,236),\t(236,238),\t(238,240),\t(240,242),\t(242,244),\t(244,249),\t(249,251),\t(251,253),\t(253,255),\t(255,260),\t(260,262),\t(262,264),\t(264,266),\t(266,268),\t(268,270),\t(270,275),\t(275,277),\t(277,279),\t(279,281),\t(281,286),\t(286,288),\t(288,290),\t(290,292),\t(292,294),\t(294,297),\t(297,305),\t(305,313),\t(313,316),\t(316,319),\t(319,329),\t(329,330)]\n",
    "col_names = [\"pvar1\",\t\"pvar2\",\t\"pvar3\",\t\"pvar4\",\t\"pvar5\",\t\"pvar6\",\t\"pvar7\",\t\"pvar8\",\t\"pvar9\",\t\"pvar10\",\t\"pvar11\",\t\"pvar12\",\t\"pvar13\",\t\"pvar14\",\t\"pvar15\",\t\"pvar16\",\t\"pvar17\",\t\"pvar18\",\t\"pvar19\",\t\"pvar20\",\t\"pvar21\",\t\"pvar22\",\t\"pvar23\",\t\"pvar24\",\t\"pvar25\",\t\"pvar26\",\t\"pvar27\",\t\"pvar28\",\t\"pvar29\",\t\"pvar30\",\t\"pvar31\",\t\"pvar32\",\t\"pvar33\",\t\"pvar34\",\t\"pvar35\",\t\"pvar36\",\t\"pvar37\",\t\"pvar38\",\t\"pvar39\",\t\"pvar40\",\t\"pvar41\",\t\"pvar42\",\t\"pvar43\",\t\"pvar44\",\t\"pvar45\",\t\"pvar46\",\t\"pvar47\",\t\"pvar48\",\t\"pvar49\",\t\"pvar50\",\t\"pvar51\",\t\"pvar52\",\t\"pvar53\",\t\"pvar54\",\t\"pvar55\",\t\"pvar56\",\t\"pvar57\",\t\"pvar58\",\t\"pvar59\",\t\"pvar60\",\t\"pvar61\",\t\"pvar62\",\t\"pvar63\",\t\"pvar64\",\t\"pvar65\",\t\"pvar66\",\t\"pvar67\",\t\"pvar68\",\t\"pvar69\",\t\"pvar70\",\t\"pvar71\",\t\"pvar72\",\t\"pvar73\",\t\"pvar74\",\t\"pvar75\",\t\"pvar76\",\t\"pvar77\",\t\"pvar78\",\t\"pvar79\",\t\"pvar80\",\t\"pvar81\",\t\"pvar82\",\t\"pvar83\",\t\"pvar84\",\t\"pvar85\",\t\"pvar86\",\t\"pvar87\",\t\"pvar88\",\t\"pvar89\",\t\"pvar90\",\t\"pvar91\",\t\"pvar92\",\t\"pvar93\",\t\"pvar94\",\t\"pvar95\",\t\"pvar96\",\t\"pvar97\",\t\"pvar98\",\t\"pvar99\",\t\"pvar100\",\t\"pvar101\",\t\"pvar102\",\t\"pvar103\",\t\"pvar104\",\t\"pvar105\",\t\"pvar106\",\t\"pvar107\",\t\"pvar108\",\t\"pvar109\",\t\"pvar110\",\t\"pvar111\",\t\"pvar112\",\t\"pvar113\",\t\"pvar114\",\t\"pvar115\",\t\"pvar116\",\t\"pvar117\",\t\"pvar118\",\t\"pvar119\",\t\"pvar120\",\t\"pvar121\",\t\"pvar122\",\t\"pvar123\",\t\"pvar124\",\t\"pvar125\",\t\"pvar126\",\t\"pvar127\",\t\"pvar128\",\t\"pvar129\",\t\"pvar130\",\t\"pvar131\",\t\"pvar132\",\t\"pvar133\",\t\"pvar134\",\t\"pvar135\",\t\"pvar136\",\t\"pvar137\",\t\"pvar138\",\t\"pvar139\"]\n",
    "col_desc = [\"File Identification\",\t\"Schdule\",\t\"Quarter\",\t\"Visit\",\t\"Sector\",\t\"State/Ut Code\",\t\"District Code\",\t\"NSS-Region\",\t\"Stratum\",\t\"Sub-Stratum\",\t\"Sub-Sample\",\t\"Fod Sub-Region\",\t\"FSU\",\t\"Sample Sg/Sb No.\",\t\"Second Stage Stratum No.\",\t\"Sample Household Number\",\t\"Person Serial No.\",\t\"Relationship To Head\",\t\"Gender\",\t\"Age\",\t\"Marital Status\",\t\"General Educaion Level\",\t\"Technical Educaion Level\",\t\"No. of years in Formal Education\",\t\"Status of Current Attendance in Educational Institution\",\t\"Whether received any Vocational/Technical Training\",\t\"Whether Training completed during last 365 Days\",\t\"Field Of Training\",\t\"Duration Of Training\",\t\"Type Of Training\",\t\"Source Of Funding The Training\",\t\"Status Code\",\t\"Industry Code (NIC)\",\t\"Occupation Code (NCO)\",\t\"Whether Engaged In Any Work In Subsidiary Capacity\",\t\"(Principal)location Of Workplace Code\",\t\"(Principal) Enterprise Type Code\",\t\"(Principal) No. Of Workers In The Enterprise\",\t\"(Principal)  Type Of Job Contract\",\t\"(Principal) Eligble Of Paid Leave\",\t\"(Principal) Social Security Benefits\",\t\"(Principal) Usage of product of the economic activity\",\t\"Status Code\",\t\"Industry Code (NIC)\",\t\"Occupation Code (NCO)\",\t\"(Subsidiary) location Of Workplace Code\",\t\"(Subsidiary)  Enterprise Type Code\",\t\"(Subsidiary)  No. Of Workers In The Enterprise\",\t\"(Subsidiary)   Type Of Job Contract\",\t\"(Subsidiary)  Eligble Of Paid Leave\",\t\"(Subsidiary)  Social Security Benefits\",\t\"(Subsidiary) Usage of product of the economic activity\",\t\"Ever Worked Prior to last 365 days\",\t\"Duration of engagement in the economic activity in usual Principal Activity Status\",\t\"Duration of engagement in the economic activity in Subsidiary Activity Status\",\t\"Efforts undertaken to search work\",\t\"Duration of spell of Unemployment\",\t\"Whether Ever Worked \",\t\" Reason for not working in last 365 days\",\t\"Main reason for being in Principal activity status (91 to 97) \",\t\"Status Code for activity 1\",\t\"Industry Code (NIC) for activity 1\",\t\"hours actuallly worked for activity 1 on 7 th day\",\t\"wage earning for activity 1 on 7 th day\",\t\"Status Code for activity 2\",\t\"Industry Code (NIC) for activity 2\",\t\"hours actuallly worked for activity 2 on 7 th day\",\t\"wage earning for activity 2 on 7 th day\",\t\"total hours actually worked on 7th day\",\t\"hours available for aditional worked on 7th day\",\t\"Status Code for activity 1\",\t\"Industry Code (NIC) for activity 1\",\t\"hours actuallly worked for activity 1 on 6 th day\",\t\"wage earning for activity 1 on 7 th day\",\t\"Status Code for activity 2\",\t\"Industry Code (NIC) for activity 2\",\t\"hours actuallly worked for activity 2 on 6 th day\",\t\"wage earning for activity 2 on 6 th day\",\t\"total hours actually worked on 6th day\",\t\"hours available for aditional worked on 6th day\",\t\"Status Code for activity 1\",\t\"Industry Code (NIC) for activity 1\",\t\"hours actuallly worked for activity 1 on5 th day\",\t\"wage earning for activity 1 on 5 th day\",\t\"Status Code for activity 2\",\t\"Industry Code (NIC) for activity 2\",\t\"hours actuallly worked for activity 2 on 5 th day\",\t\"wage earning for activity 2 on 5 th day\",\t\"total hours actually worked on 5th day\",\t\"hours available for aditional worked on 5th day\",\t\"Status Code for activity 1\",\t\"Industry Code (NIC) for activity 1\",\t\"hours actuallly worked for activity 1 on 4th day\",\t\"wage earning for activity 1 on 4th day\",\t\"Status Code for activity 2\",\t\"Industry Code (NIC) for activity 2\",\t\"hours actuallly worked for activity 2 on 4th day\",\t\"wage earning for activity 2 on 4th day\",\t\"total hours actually worked on 4th day\",\t\"hours available for aditional worked on 4th day\",\t\"Status Code for activity 1\",\t\"Industry Code (NIC) for activity 1\",\t\"hours actuallly worked for activity 1 on 3rd day\",\t\"wage earning for activity 1 on 3rd day\",\t\"Status Code for activity 2\",\t\"Industry Code (NIC) for activity 2\",\t\"hours actuallly worked for activity 2 on 3rd day\",\t\"wage earning for activity 2 on 3 rd day\",\t\"total hours actually worked on 3rd day\",\t\"hours available for aditional worked on 3rd day\",\t\"Status Code for activity 1\",\t\"Industry Code (NIC) for activity 1\",\t\"hours actuallly worked for activity 1 on 2nd day\",\t\"wage earning for activity 1 on 2nd day\",\t\"Status Code for activity 2\",\t\"Industry Code (NIC) for activity 2\",\t\"hours actuallly worked for activity 2 on 2nd day\",\t\"wage earning for activity 2 on 2nd day\",\t\"total hours actually worked on 2nd day\",\t\"hours available for aditional worked on 2nd day\",\t\"Status Code for activity 1\",\t\"Industry Code (NIC) for activity 1\",\t\"hours actuallly worked for activity 1 on 1st day\",\t\"wage earning for activity 1 on 1st day\",\t\"Status Code for activity 2\",\t\"Industry Code (NIC) for activity 2\",\t\"hours actuallly worked for activity 2 on 1st day\",\t\"wage earning for activity 2 on 1st day\",\t\"total hours actually worked on 1st day\",\t\"hours available for aditional worked on 1st day\",\t\"Current Weekly Status (CWS)\",\t\"Industry Code (CWS)\",\t\"Occupation Code (CWS)\",\t\"Earnings For Regular Salaried/Wage Activity\",\t\"Earnings For Self Employed\",\t\"Ns count for sector x stratum x substratum x sub-sample\",\t\"Ns count for sector x stratum x substratum\",\t\"Sub-sample wise Multiplier\",\t\"Count of contributing State x Sector x Stratum x SubStratum in 4 Quarters\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiating PLFS dataloader with hh files\n",
    "df_perv1,coldesc_perv1 = PLFSdataloader(file_path = file_path, col_names = col_names, col_desc = col_desc, col_specs = col_specs).get_dataframes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pvar1</th>\n",
       "      <th>pvar2</th>\n",
       "      <th>pvar3</th>\n",
       "      <th>pvar4</th>\n",
       "      <th>pvar5</th>\n",
       "      <th>pvar6</th>\n",
       "      <th>pvar7</th>\n",
       "      <th>pvar8</th>\n",
       "      <th>pvar9</th>\n",
       "      <th>pvar10</th>\n",
       "      <th>...</th>\n",
       "      <th>pvar130</th>\n",
       "      <th>pvar131</th>\n",
       "      <th>pvar132</th>\n",
       "      <th>pvar133</th>\n",
       "      <th>pvar134</th>\n",
       "      <th>pvar135</th>\n",
       "      <th>pvar136</th>\n",
       "      <th>pvar137</th>\n",
       "      <th>pvar138</th>\n",
       "      <th>pvar139</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FVP7</td>\n",
       "      <td>104</td>\n",
       "      <td>Q1</td>\n",
       "      <td>V1</td>\n",
       "      <td>1</td>\n",
       "      <td>02</td>\n",
       "      <td>04</td>\n",
       "      <td>021</td>\n",
       "      <td>01</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>246798</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FVP7</td>\n",
       "      <td>104</td>\n",
       "      <td>Q1</td>\n",
       "      <td>V1</td>\n",
       "      <td>1</td>\n",
       "      <td>02</td>\n",
       "      <td>04</td>\n",
       "      <td>021</td>\n",
       "      <td>01</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>246798</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FVP7</td>\n",
       "      <td>104</td>\n",
       "      <td>Q1</td>\n",
       "      <td>V1</td>\n",
       "      <td>1</td>\n",
       "      <td>02</td>\n",
       "      <td>04</td>\n",
       "      <td>021</td>\n",
       "      <td>01</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>01</td>\n",
       "      <td>611</td>\n",
       "      <td>0</td>\n",
       "      <td>6500</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>246798</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FVP7</td>\n",
       "      <td>104</td>\n",
       "      <td>Q1</td>\n",
       "      <td>V1</td>\n",
       "      <td>1</td>\n",
       "      <td>02</td>\n",
       "      <td>04</td>\n",
       "      <td>021</td>\n",
       "      <td>01</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>01</td>\n",
       "      <td>611</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>246798</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FVP7</td>\n",
       "      <td>104</td>\n",
       "      <td>Q1</td>\n",
       "      <td>V1</td>\n",
       "      <td>1</td>\n",
       "      <td>02</td>\n",
       "      <td>04</td>\n",
       "      <td>021</td>\n",
       "      <td>01</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>246798</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  pvar1 pvar2 pvar3 pvar4 pvar5 pvar6 pvar7 pvar8 pvar9 pvar10  ... pvar130  \\\n",
       "0  FVP7   104    Q1    V1     1    02    04   021    01     14  ...       0   \n",
       "1  FVP7   104    Q1    V1     1    02    04   021    01     14  ...       0   \n",
       "2  FVP7   104    Q1    V1     1    02    04   021    01     14  ...       0   \n",
       "3  FVP7   104    Q1    V1     1    02    04   021    01     14  ...       3   \n",
       "4  FVP7   104    Q1    V1     1    02    04   021    01     14  ...       0   \n",
       "\n",
       "  pvar131 pvar132 pvar133 pvar134 pvar135 pvar136 pvar137 pvar138 pvar139  \n",
       "0      94     NaN     NaN       0       0       2       4  246798       4  \n",
       "1      93     NaN     NaN       0       0       2       4  246798       4  \n",
       "2      11      01     611       0    6500       2       4  246798       4  \n",
       "3      21      01     611       0       0       2       4  246798       4  \n",
       "4      91     NaN     NaN       0       0       2       4  246798       4  \n",
       "\n",
       "[5 rows x 139 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_perv1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging household and personal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating hhid in HHV1\n",
    "df_hhv1[\"hhid\"] = df_hhv1['hhvar3'] + df_hhv1['hhvar4'] + df_hhv1['hhvar5'] + df_hhv1['hhvar13']+ df_hhv1['hhvar14']+ df_hhv1['hhvar15']+ df_hhv1['hhvar16']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating hhid in PERV1\n",
    "df_perv1[\"hhid\"] = df_perv1['pvar3'] + df_perv1['pvar4'] + df_perv1['pvar5'] + df_perv1['pvar13'] + df_perv1['pvar14'] + df_perv1['pvar15'] + df_perv1['pvar16']\n",
    "\n",
    "#Creatinn perid in PERV1\n",
    "df_perv1[\"perid\"] = df_perv1['pvar3'] + df_perv1['pvar4'] + df_perv1['pvar5'] + df_perv1['pvar13'] + df_perv1['pvar14'] + df_perv1['pvar15'] + df_perv1['pvar16'] + df_perv1['pvar17']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_merge\n",
       "both          418159\n",
       "left_only          0\n",
       "right_only         0\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Merging HHV1 and PERV1\n",
    "df_merged = pd.merge(df_hhv1,df_perv1, on = \"hhid\", how=\"outer\",indicator= True)\n",
    "df_merged[\"_merge\"].value_counts() #Checking for unmerged rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating annual weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     418159\n",
      "unique         2\n",
      "top            4\n",
      "freq      418049\n",
      "Name: pvar139, dtype: object\n",
      "['4' '3']\n"
     ]
    }
   ],
   "source": [
    "print(df_merged[\"pvar139\"].describe())\n",
    "print(df_merged[\"pvar139\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating weights for first visits\n",
    "\n",
    "## Changing dtype to numeric\n",
    "df_merged[['pvar138','pvar139','pvar136','pvar137']] = df_merged[['pvar138','pvar139','pvar136','pvar137']].apply(pd.to_numeric, errors = 'coerce')\n",
    "\n",
    "## Assigning weights \n",
    "df_merged[\"weights\"] = df_merged.apply(lambda row: round(row['pvar138']/(row['pvar139']*100) if row['pvar136'] == row['pvar137'] else row['pvar138']/(row['pvar139']*200)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1204324589)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Total population as per PLFS\n",
    "df_merged[\"weights\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting files in dta format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_stata(r\"Extracted_files\\HHV1_PERV1_merged.dta\", write_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coldesc_hhv1.to_csv(r\"..\\Extracted_files\\hhv1 column description.csv\")\n",
    "coldesc_perv1.to_csv(r\"..\\Extracted_files\\perv1 column descriptions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envl15nov",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
